{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from typing import Callable\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from radar_maps.env.radar_map_double_integrator import RadarMap_DoubleIntegrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_utils import ActorNet\n",
    "from critic_utils import CriticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_obs(x):\n",
    "    # print(x.shape)\n",
    "    obs = {}\n",
    "    img = torch.clone(x[:, 0:14400])\n",
    "    # print(\"img shape: \", img.shape)\n",
    "    state = torch.clone(x[:, -4:])\n",
    "    obs['img'] = img.view(x.shape[0], 1, 120, 120)\n",
    "    obs['state'] = state.view(x.shape[0], 4)\n",
    "    return obs\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "def make_env(env_id, idx, capture_video, run_name, gamma):\n",
    "    map_size = 1000\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id,\n",
    "                            map_size = map_size, \n",
    "                            goal_location=[map_size, map_size], \n",
    "                            radar_detection_range=300,\n",
    "                            grid_size=5,\n",
    "                            dist_between_radars=map_size/5.0,\n",
    "                            num_radars=10)\n",
    "        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        # env = gym.wrappers.NormalizeObservation(env)\n",
    "        # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        return env\n",
    "\n",
    "    return thunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "torch_deterministic = True\n",
    "env_id = \"RadarMap-DoubleIntegrator-v0\"\n",
    "exp_name = \"rl_finetune\"\n",
    "capture_video = False\n",
    "run_name = run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "gamma = 0.99\n",
    "num_envs = 1\n",
    "learning_rate = 3e-4\n",
    "num_steps = 2048\n",
    "total_timesteps = 100\n",
    "batch_size = int(num_envs * num_steps)\n",
    "anneal_lr = True\n",
    "target_kl = None\n",
    "max_grad_norm = 0.5\n",
    "vf_coef = 0.5\n",
    "ent_coef = 0.0\n",
    "clip_coef = 0.2\n",
    "clip_vloss = True\n",
    "gae_lambda = 0.95\n",
    "num_minibatches = 32\n",
    "norm_adv = True\n",
    "update_epochs = 10\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following path to point to the agent trained from imitation learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agant_path = \"/home/lucas/Documents/GitHub/mpc_imitation_learning/agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/Documents/GitHub/mpc_imitation_learning/imitation_learning_env/lib/python3.10/site-packages/gym-0.26.2-py3.10.egg/gym/utils/passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: float32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, i, capture_video, run_name, gamma) for i in range(num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "dummy_env = RadarMap_DoubleIntegrator(1000, [1000, 1000], 300, 5, 200, 10)\n",
    "actor = ActorNet(dummy_env.action_space, dummy_env.observation_space, hidden_sizes=[64, 64])\n",
    "actor.load_state_dict(torch.load(agant_path))\n",
    "critic = CriticNet(actor.feature_extractor, [64, 64])\n",
    "actor.to(device)\n",
    "critic.to(device)\n",
    "\n",
    "all_params = list(actor.parameters()) + list(critic.parameters())\n",
    "all_params = dict.fromkeys(all_params).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Freeze weights of actor net to train critic.\n",
    "'''\n",
    "for name, param in actor.named_parameters():\n",
    "        param.requires_grad = False\n",
    "optimizer = optim.Adam(all_params, lr=learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for update in range(1, num_updates + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            obs_unflattened = unflatten_obs(next_obs)\n",
    "            # print(\"Obs: \", next_obs.shape)\n",
    "            action, act_dist = actor.forward(obs_unflattened)\n",
    "            logprob = act_dist.log_prob(action)\n",
    "            value = critic.forward(obs_unflattened)\n",
    "\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap value if not done\n",
    "with torch.no_grad():\n",
    "    # next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "    next_value = critic.forward(unflatten_obs(next_obs))\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - dones[t + 1]\n",
    "            nextvalues = values[t + 1]\n",
    "        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "    returns = advantages + values\n",
    "\n",
    "# flatten the batch\n",
    "b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "b_logprobs = logprobs.reshape(-1)\n",
    "b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "b_advantages = advantages.reshape(-1)\n",
    "b_returns = returns.reshape(-1)\n",
    "b_values = values.reshape(-1)\n",
    "\n",
    "# Optimizing the policy and value network\n",
    "b_inds = np.arange(batch_size)\n",
    "clipfracs = []\n",
    "for epoch in range(update_epochs):\n",
    "    np.random.shuffle(b_inds)\n",
    "    for start in range(0, batch_size, minibatch_size):\n",
    "        end = start + minibatch_size\n",
    "        mb_inds = b_inds[start:end]\n",
    "\n",
    "        # _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "        obs_unflattened = unflatten_obs(b_obs[mb_inds])\n",
    "        _, act_dist = actor.forward(obs_unflattened)\n",
    "        newlogprob = act_dist.log_prob(b_actions[mb_inds])\n",
    "        entropy = act_dist.entropy()\n",
    "        newvalue = critic.forward(obs_unflattened)\n",
    "\n",
    "        logratio = newlogprob - b_logprobs[mb_inds]\n",
    "        ratio = logratio.exp()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "            old_approx_kl = (-logratio).mean()\n",
    "            approx_kl = ((ratio - 1) - logratio).mean()\n",
    "            clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "        mb_advantages = b_advantages[mb_inds]\n",
    "        if norm_adv:\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "        # Policy loss\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "        # Value loss\n",
    "        newvalue = newvalue.view(-1)\n",
    "        if clip_vloss:\n",
    "            v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "            v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                newvalue - b_values[mb_inds],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "        else:\n",
    "            v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(all_params, max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    if target_kl is not None:\n",
    "        if approx_kl > target_kl:\n",
    "            break\n",
    "\n",
    "y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "var_y = np.var(y_true)\n",
    "explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following to where you wish to store the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"/home/lucas/Documents/GitHub/mpc_imitation_learning/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /home/lucas/Documents/GitHub/mpc_imitation_learning/runs/RadarMap-DoubleIntegrator-v0__rl_finetune__1__1711134596 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lucas/Documents/GitHub/mpc_imitation_learning/rl_finetune.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/Documents/GitHub/mpc_imitation_learning/rl_finetune.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m save_model:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lucas/Documents/GitHub/mpc_imitation_learning/rl_finetune.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(actor\u001b[39m.\u001b[39;49mstate_dict(), model_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/actor\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/Documents/GitHub/mpc_imitation_learning/rl_finetune.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(critic\u001b[39m.\u001b[39mstate_dict(), model_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/critic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/Documents/GitHub/mpc_imitation_learning/rl_finetune.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel saved to \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/mpc_imitation_learning/imitation_learning_env/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/mpc_imitation_learning/imitation_learning_env/lib/python3.10/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/Documents/GitHub/mpc_imitation_learning/imitation_learning_env/lib/python3.10/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /home/lucas/Documents/GitHub/mpc_imitation_learning/runs/RadarMap-DoubleIntegrator-v0__rl_finetune__1__1711134596 does not exist."
     ]
    }
   ],
   "source": [
    "if save_model:\n",
    "    torch.save(actor.state_dict(), model_path + '/actor')\n",
    "    torch.save(critic.state_dict(), model_path + '/critic')\n",
    "    print(f\"model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
