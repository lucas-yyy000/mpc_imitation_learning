{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyrl\n",
    "from easyrl.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "from radar_maps.env.radar_map_double_integrator import RadarMap_DoubleIntegrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"double_integrator/data_multimodal/\"\n",
    "data_num = 4\n",
    "num_mode = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self, obs, actions):\n",
    "        self.obs = obs # Observations\n",
    "        self.actions = actions # Actions\n",
    "\n",
    "        \n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, trajs):\n",
    "        states = []\n",
    "        actions = []\n",
    "        for traj in trajs:\n",
    "            states.extend(traj.obs)\n",
    "            actions.extend(traj.actions)\n",
    "\n",
    "        self.states = np.array(states)\n",
    "        self.actions = np.array(actions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.actions.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "        sample['state'] = self.states[idx]['state']\n",
    "        sample['img'] = self.states[idx]['img']\n",
    "        sample['action'] = self.actions[idx]\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def get_path_chunk(path, num_points=3):\n",
    "    len_path = len(path)\n",
    "    if len_path >= num_points:\n",
    "        return list(np.hstack(path[:num_points]))\n",
    "    \n",
    "    path_patched = list(np.hstack(path))\n",
    "    for _ in range(num_points-len_path):\n",
    "        path_patched.extend(list(path[-1]))\n",
    "    # print(\"Patched path: \", path_patched)\n",
    "    return path_patched\n",
    "\n",
    "# @jit\n",
    "def get_radar_heat_map(state, radar_locs, img_size, radar_detection_range, grid_size):\n",
    "    radars_encoding = np.zeros((img_size, img_size))\n",
    "    theta = np.arctan2(state[3], state[1])\n",
    "    loc_to_glob = np.array([[np.cos(theta), -np.sin(theta), state[0]],\n",
    "                            [np.sin(theta), np.cos(theta), state[2]],\n",
    "                            [0., 0., 1.]])\n",
    "    glob_to_loc = np.linalg.inv(loc_to_glob)\n",
    "    # print(glob_to_loc)\n",
    "    for radar_loc in radar_locs:\n",
    "        if abs(state[0] - radar_loc[0]) < radar_detection_range or abs(state[2] - radar_loc[1]) < radar_detection_range:\n",
    "            glob_loc_hom = np.array([radar_loc[0], radar_loc[1], 1])\n",
    "            local_loc_hom = np.dot(glob_to_loc, glob_loc_hom)\n",
    "            radars_loc_coord = local_loc_hom[:2]\n",
    "\n",
    "            y_grid = np.rint((radars_loc_coord[1]) / grid_size) \n",
    "            x_grid = np.rint((radars_loc_coord[0]) / grid_size) \n",
    "\n",
    "            for i in range(-int(img_size/2), int(img_size/2)):\n",
    "                for j in range(-int(img_size/2), int(img_size/2)):\n",
    "                    radars_encoding[int(i + img_size/2), int(j + img_size/2)] += np.exp((-(x_grid - i)**2 - (y_grid - j)**2))*1e3\n",
    "\n",
    "    radars_encoding = radars_encoding.T\n",
    "    if np.max(radars_encoding) > 0:\n",
    "        formatted = (radars_encoding * 255.0 / np.max(radars_encoding)).astype('float32')\n",
    "    else:\n",
    "        formatted = radars_encoding.astype('float32')\n",
    "\n",
    "    formatted = formatted[np.newaxis, :, :]\n",
    "\n",
    "    return formatted\n",
    "\n",
    "# @jit\n",
    "def generate_training_data(traj, ctr, path_mm, radars, detection_range, grid_size, v_lim, u_lim):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    # print(\"Len:\", traj.shape)\n",
    "    for i in range(traj.shape[0]-1):\n",
    "        x_cur = traj[i]\n",
    "\n",
    "        heat_map_img = get_radar_heat_map(x_cur, radars, 2*int(detection_range/grid_size), detection_range, grid_size)\n",
    "        # print(heat_map_img.shape)\n",
    "        x_cur_normalized = np.array([x_cur[0]/1200.0, x_cur[1]/v_lim, x_cur[2]/1200.0, x_cur[3]/v_lim])\n",
    "\n",
    "        observation = {\"state\": x_cur_normalized, \"img\": heat_map_img}\n",
    "        observations.append(observation)\n",
    "        # print(\"Iter: \", i)\n",
    "        # print(\"Observation: \", observations)\n",
    "        if i < traj.shape[0] - 1:\n",
    "            action_prediction = []\n",
    "            for m in range(num_mode):\n",
    "                action_prediction.extend(ctr[i, 2*m:2*(m+1)]/u_lim)\n",
    "                path_tmp = path_mm[num_mode*i + m]\n",
    "                path_tmp = [x / 1200.0 for x in path_tmp]\n",
    "                # print(path_tmp)\n",
    "                action_prediction.extend(get_path_chunk(path_tmp))\n",
    "            actions.append(action_prediction)\n",
    "    # print(\"Observation shape: \", np.array(observations).shape)\n",
    "    return np.array(observations), np.array(actions)\n",
    "\n",
    "def process_data(detection_range, grid_size, v_lim, u_lim):\n",
    "    bc_data = []\n",
    "    for i in range(data_num):\n",
    "        print(\"Processing data: \", i)\n",
    "        traj = np.load(data_path + f'state_traj_{i}.npy')\n",
    "        control = np.load(data_path + f'control_traj_{i}.npy')\n",
    "        radar_config = np.load(data_path + f'radar_config_{i}.npy')\n",
    "\n",
    "        with open(data_path+ f'nominal_path_multimodal_{i}.pkl', 'rb') as handle:\n",
    "            path_mm = pickle.load(handle)\n",
    "\n",
    "        obs, acts = generate_training_data(traj, control, path_mm, radar_config, detection_range, grid_size, v_lim, u_lim)\n",
    "        bc_traj = Trajectory(obs, acts)\n",
    "        bc_data.append(bc_traj)\n",
    "    return np.array(bc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# set random seed\n",
    "seed = 0\n",
    "set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyrl import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_configs(exp_name='bc'):\n",
    "    configs.set_config('ppo')\n",
    "    configs.cfg.alg.seed = seed\n",
    "    configs.cfg.alg.num_envs = 1\n",
    "    configs.cfg.alg.episode_steps = 150\n",
    "    configs.cfg.alg.max_steps = 600000\n",
    "    configs.cfg.alg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # configs.cfg.alg.device = 'cpu'\n",
    "    configs.cfg.alg.env_name = 'RadarMap-DoubleIntegrator-v0'\n",
    "    configs.cfg.alg.save_dir = Path.cwd().absolute().joinpath('data').as_posix()\n",
    "    configs.cfg.alg.save_dir += f'/{exp_name}'\n",
    "    setattr(configs.cfg.alg, 'diff_cfg', dict(save_dir=configs.cfg.alg.save_dir))\n",
    "\n",
    "    print(f'====================================')\n",
    "    print(f'      Device:{configs.cfg.alg.device}')\n",
    "    print(f'====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO]\u001b[0m[2024-03-04 13:50:58]: \u001b[32mAlogrithm type:<class 'easyrl.configs.ppo_config.PPOConfig'>\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "      Device:cpu\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "set_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_to_tensor(array, **kwargs):\n",
    "    if isinstance(array, torch.Tensor):\n",
    "        return array\n",
    "\n",
    "    if not array.flags.writeable:\n",
    "        array = array.copy()\n",
    "\n",
    "    return torch.as_tensor(array, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc_agent(agent, trajs, max_epochs=5000, batch_size=32, lr=0.001, disable_tqdm=True, entropy_weight=1e-3):\n",
    "    dataset = TrajDataset(trajs)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            drop_last=True)\n",
    "    # print(\"Dataset shape \", dataset.actions.shape)\n",
    "    optimizer = optim.Adam(agent.parameters(),\n",
    "                           lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    pbar = tqdm(range(max_epochs), desc='Epoch', disable=disable_tqdm)\n",
    "    logs = dict(loss=[], epoch=[])\n",
    "    \n",
    "    for iter in pbar:\n",
    "        avg_loss = []\n",
    "        for batch_idx, sample in enumerate(dataloader):\n",
    "            states = sample['state'].float().to(configs.cfg.alg.device)\n",
    "            imgs = sample['img'].float().to(configs.cfg.alg.device)\n",
    "            expert_actions = sample['action'].float().to(configs.cfg.alg.device)\n",
    "            optimizer.zero_grad()\n",
    "            # print(expert_actions.shape)\n",
    "            _, act_dist = agent.forward({'state': states, 'img': imgs})\n",
    "            \n",
    "            log_prob = act_dist.log_prob(expert_actions)\n",
    "            log_prob = log_prob.mean()\n",
    "\n",
    "            entropy = act_dist.entropy()\n",
    "            entropy = entropy.mean() if entropy is not None else None\n",
    "            entropy_loss = -entropy_weight * (entropy if entropy is not None else torch.zeros(1))\n",
    "            # print(actions)\n",
    "            '''\n",
    "            Handle trajectory assignment.\n",
    "            '''\n",
    "            indx1 = range(8)\n",
    "            indx2 = range(8, 16)\n",
    "            indx3 = range(16, 24)\n",
    "            indx = [indx1, indx2, indx3]\n",
    "            perm_indx = itertools.permutations([0, 1, 2], 3) \n",
    "            # print(\"Expert actions: \", expert_actions[0])\n",
    "            for ind in list(perm_indx):\n",
    "                indices = []\n",
    "                indices.extend(indx[ind[0]])\n",
    "                indices.extend(indx[ind[1]])\n",
    "                indices.extend(indx[ind[2]])\n",
    "                log_prob_permutated = act_dist.log_prob(expert_actions[:,  torch.tensor(indices)])\n",
    "                log_prob_permutated = log_prob_permutated.mean()\n",
    "                # print(loss_permutated.grad_fn)\n",
    "                if log_prob_permutated > log_prob:\n",
    "                    log_prob = torch.clone(log_prob_permutated)\n",
    "            ####\n",
    "            neglogp = -log_prob\n",
    "            loss = neglogp + entropy_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            avg_loss.append(loss.item())\n",
    "        \n",
    "            # print(\"Optimizer param: \", optimizer.param_groups)\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {iter} Loss: ', np.mean(avg_loss))\n",
    "        logs['loss'].append(np.mean(avg_loss))\n",
    "        logs['epoch'].append(iter)\n",
    "    return agent, logs, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats.qmc' has no attribute 'PoissonDisk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8fee5f68b1bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdetection_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRadarMap_DoubleIntegrator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_of_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize_of_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_of_map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_between_radars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_of_map\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_radars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/pose_aware_planning/imitation_learning/radar_maps/env/radar_map_double_integrator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, map_size, goal_location, radar_detection_range, grid_size, dist_between_radars, num_radars, time_step, v, u_lim)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mradar_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoronoi_diagram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplanning_on_voronoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_baseline_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_between_radars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_between_radars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_radar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_radars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradar_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradar_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/pose_aware_planning/imitation_learning/planning_on_voronoi.py\u001b[0m in \u001b[0;36mget_baseline_path\u001b[0;34m(size, dist_between_radars, num_radar)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mradius\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_between_radars\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoissonDisk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msample_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_radar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.stats.qmc' has no attribute 'PoissonDisk'"
     ]
    }
   ],
   "source": [
    "size_of_map = 1000\n",
    "detection_range = 300\n",
    "grid_size = 5\n",
    "env = RadarMap_DoubleIntegrator(size_of_map, [size_of_map, size_of_map], detection_range, grid_size, dist_between_radars=size_of_map/5.0, num_radars=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_utils import ActorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorNet(env.action_space, env.observation_space, [64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorNet(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (extractors): ModuleDict(\n",
      "      (img): NatureCNN(\n",
      "        (cnn): Sequential(\n",
      "          (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "          (5): ReLU()\n",
      "          (6): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "        (linear): Sequential(\n",
      "          (0): Linear(in_features=7744, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (state): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (policy_net): Sequential(\n",
      "    (0): Linear(in_features=260, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data:  0\n",
      "Processing data:  1\n",
      "Processing data:  2\n",
      "Processing data:  3\n"
     ]
    }
   ],
   "source": [
    "bc_data = process_data(detection_range, grid_size, 20.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module, gain: float = 1) -> None:\n",
    "    \"\"\"\n",
    "    Orthogonal initialization (used in PPO and A2C)\n",
    "    \"\"\"\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.orthogonal_(module.weight, gain=gain)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_gains = {\n",
    "    agent.feature_extractor: np.sqrt(2),\n",
    "    agent.policy_net: np.sqrt(2),\n",
    "    agent.action_net: 0.01,\n",
    "}\n",
    "\n",
    "for module, gain in module_gains.items():\n",
    "    module.apply(partial(init_weights, gain=gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    agent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(bc_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss:  23.26210525708321\n",
      "Epoch 1 Loss:  21.476908903855545\n",
      "Epoch 2 Loss:  20.281897520407654\n",
      "Epoch 3 Loss:  19.44217892182179\n",
      "Epoch 4 Loss:  18.713183916532078\n",
      "Epoch 5 Loss:  18.079755098391804\n",
      "Epoch 6 Loss:  17.52592932872283\n",
      "Epoch 7 Loss:  17.029985330043694\n",
      "Epoch 8 Loss:  16.594215295253655\n",
      "Epoch 9 Loss:  16.202442560440456\n",
      "Epoch 10 Loss:  15.845841285509941\n",
      "Epoch 11 Loss:  15.528942841749926\n",
      "Epoch 12 Loss:  15.248218634189703\n",
      "Epoch 13 Loss:  14.99305177346254\n",
      "Epoch 14 Loss:  14.755873900193434\n",
      "Epoch 15 Loss:  14.53539202763484\n",
      "Epoch 16 Loss:  14.33320783957457\n",
      "Epoch 17 Loss:  14.145398017687675\n",
      "Epoch 18 Loss:  13.97642402159862\n",
      "Epoch 19 Loss:  13.824726691612831\n"
     ]
    }
   ],
   "source": [
    "agent, logs, _= train_bc_agent(agent, bc_data, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNet(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (extractors): ModuleDict(\n",
       "      (img): NatureCNN(\n",
       "        (cnn): Sequential(\n",
       "          (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "          (5): ReLU()\n",
       "          (6): Flatten(start_dim=1, end_dim=-1)\n",
       "        )\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=7744, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (state): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (policy_net): Sequential(\n",
       "    (0): Linear(in_features=260, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization\n",
    "import planning_on_voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import warnings\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions from stable-baseline3.\n",
    "See:\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/1cba1bbd2f129f3e3140d6a1e478dd4b3979a2bf/stable_baselines3/common/preprocessing.py\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/1cba1bbd2f129f3e3140d6a1e478dd4b3979a2bf/stable_baselines3/common/utils.py\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/1cba1bbd2f129f3e3140d6a1e478dd4b3979a2bf/stable_baselines3/common/policies.py\n",
    "'''\n",
    "def is_image_space_channels_first(observation_space: spaces.Box) -> bool:\n",
    "    smallest_dimension = np.argmin(observation_space.shape).item()\n",
    "    if smallest_dimension == 1:\n",
    "        warnings.warn(\"Treating image space as channels-last, while second dimension was smallest of the three.\")\n",
    "    return smallest_dimension == 0\n",
    "\n",
    "\n",
    "def is_image_space(\n",
    "    observation_space: spaces.Space,\n",
    "    check_channels: bool = False,\n",
    "    normalized_image: bool = False,\n",
    ") -> bool:\n",
    "    check_dtype = check_bounds = not normalized_image\n",
    "    if isinstance(observation_space, spaces.Box) and len(observation_space.shape) == 3:\n",
    "        # Check the type\n",
    "        if check_dtype and observation_space.dtype != np.uint8:\n",
    "            return False\n",
    "\n",
    "        # Check the value range\n",
    "        incorrect_bounds = np.any(observation_space.low != 0) or np.any(observation_space.high != 255)\n",
    "        if check_bounds and incorrect_bounds:\n",
    "            return False\n",
    "\n",
    "        # Skip channels check\n",
    "        if not check_channels:\n",
    "            return True\n",
    "        # Check the number of channels\n",
    "        if is_image_space_channels_first(observation_space):\n",
    "            n_channels = observation_space.shape[0]\n",
    "        else:\n",
    "            n_channels = observation_space.shape[-1]\n",
    "        # GrayScale, RGB, RGBD\n",
    "        return n_channels in [1, 3, 4]\n",
    "    return False\n",
    "\n",
    "def maybe_transpose(observation: np.ndarray, observation_space: spaces.Space) -> np.ndarray:\n",
    "    # Avoid circular import\n",
    "    if is_image_space(observation_space):\n",
    "        if not (observation.shape == observation_space.shape or observation.shape[1:] == observation_space.shape):\n",
    "            # Try to re-order the channels\n",
    "            transpose_obs = transpose_image(observation)\n",
    "            if transpose_obs.shape == observation_space.shape or transpose_obs.shape[1:] == observation_space.shape:\n",
    "                observation = transpose_obs\n",
    "    return observation\n",
    "\n",
    "def transpose_image(image: np.ndarray) -> np.ndarray:\n",
    "    if len(image.shape) == 3:\n",
    "        return np.transpose(image, (2, 0, 1))\n",
    "    return np.transpose(image, (0, 3, 1, 2))\n",
    "    \n",
    "def obs_as_tensor(obs, device):\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        return torch.as_tensor(obs, device=device).float()\n",
    "    elif isinstance(obs, dict):\n",
    "        return {key: torch.as_tensor(_obs, device=device).float() for (key, _obs) in obs.items()}\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized type of observation {type(obs)}\")\n",
    "\n",
    "def obs_to_tensor(observation, observation_space, device):\n",
    "    # need to copy the dict as the dict in VecFrameStack will become a torch tensor\n",
    "    observation = copy.deepcopy(observation)\n",
    "    for key, obs in observation.items():\n",
    "        obs_space = observation_space.spaces[key]\n",
    "        if is_image_space(obs_space):\n",
    "            obs_ = maybe_transpose(obs, obs_space)\n",
    "        else:\n",
    "            obs_ = np.array(obs)\n",
    "        # Add batch dimension if needed\n",
    "        observation[key] = obs_.reshape((-1, *observation_space[key].shape))  # type: ignore[misc]\n",
    "\n",
    "    obs_tensor = obs_as_tensor(observation, device)\n",
    "    return obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test trained student policy.\n",
    "'''\n",
    "obs, _ = env.reset()\n",
    "radar_config = env.radar_locs\n",
    "print(\"Initial state: \", obs['state'])\n",
    "trajectory = []\n",
    "trajectory.append(obs['state'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1500):\n",
    "        action, _ = agent.forward(obs_to_tensor(obs, env.observation_space, configs.cfg.alg.device))\n",
    "        action = action.cpu().numpy()\n",
    "        # print(action[0])\n",
    "        action_clipped = np.clip(action[0], env.action_space.low, env.action_space.high)\n",
    "        obs, reward, done, _, _ = env.step(action[0])\n",
    "        # vec_env.render()\n",
    "        # print(\"Action: \", action)\n",
    "        trajectory.append(env.state['state'])\n",
    "        if done:\n",
    "            # print(\"Action: \", action)\n",
    "            print(\"State: \", env.state['state'])\n",
    "            break\n",
    "radar_locs, voronoi_diagram, path = planning_on_voronoi.get_baseline_path_with_vertices(radar_config, size_of_map)\n",
    "visualization.visualiza_traj(trajectory, radar_locs, voronoi_diagram, path, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
